{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import matplotlib\n",
      "import lasagne as nn\n",
      "matplotlib.use('Agg')\n",
      "import math\n",
      "import time\n",
      "import numpy as np\n",
      "import theano\n",
      "import theano.tensor as T\n",
      "from theano.sandbox.neighbours import neibs2images\n",
      "from operator import mul\n",
      "import pickle\n",
      "import sys\n",
      "import os\n",
      "from PIL import Image, ImageOps\n",
      "from numpy.random import RandomState\n",
      "from datetime import datetime\n",
      "from matplotlib import pyplot as plt\n",
      "from pandas.io.parsers import read_csv\n",
      "from mylayers import VAE_Z_Layer, MinusBiasLayer\n",
      "prng = RandomState(datetime.now().microsecond)\n",
      "sys.setrecursionlimit(10000)\n",
      "nn.random.set_rng(prng)\n",
      "\n",
      "def load(imgfolder, split=0.8, limit=1024,gray=False,enhance=True,randomize=True,\n",
      "        startfrom=0):\n",
      "    imgExts=['png', 'bmp', 'jpg']\n",
      "    X=[]\n",
      "    ct = 0\n",
      "    st = time.time()\n",
      "    fns = []\n",
      "    for path, dirs, files in os.walk(imgfolder):\n",
      "        for fn in files:\n",
      "            if fn[-3:].lower() in imgExts:\n",
      "                fns.append(fn)\n",
      "    if randomize:\n",
      "        prng.shuffle(fns)\n",
      "    else:\n",
      "        fns = sorted(fns, key=lambda f: int(f.split('.')[0].split('_')[1]))\n",
      "        fns = fns[startfrom:]\n",
      "    for fn in fns:\n",
      "        im = Image.open(os.path.join(imgfolder, fn))\n",
      "        ch = 3\n",
      "        if gray:\n",
      "            ch = 1\n",
      "            im = im.convert('L')\n",
      "        if enhance:\n",
      "            im = ImageOps.autocontrast(im)\n",
      "        w, h = im.size\n",
      "        if w % 2 != 0:\n",
      "            im = im.crop((0, 0, w-1, h))\n",
      "            w -= 1\n",
      "        if h % 2 != 0:\n",
      "            im = im.crop((0, 0, w, h-1))\n",
      "            h -= 1\n",
      "        X.append(np.asarray(im.getdata()).reshape(h,w,ch).transpose(2,1,0))\n",
      "        ct += 1\n",
      "        if ct % 1000 == 0:\n",
      "            print 'img {}: {:3f}s'.format(ct, time.time() - st)\n",
      "            st = time.time()\n",
      "        if ct > limit:\n",
      "            break\n",
      "    X = np.vstack(X).astype(np.float32).reshape(-1, ch, w, h) / 255. - 0.5\n",
      "    spl = int(X.shape[0] * split)\n",
      "    return X[:spl], X[spl:]\n",
      "\n",
      "def csize(shape, fs1, poolsize):\n",
      "    return ((shape[0] - fs1 + 1) / poolsize, (shape[1] - fs1 + 1) / poolsize)\n",
      "def softsign(x):\n",
      "    return x/(1+abs(x))\n",
      "\n",
      "def build_cae(inputvar=None, specstr='', imgshape=(100,100),channels=3,pInit=[]):\n",
      "    l_input = nn.layers.InputLayer(shape=(None,channels,imgshape[0], imgshape[1]),\n",
      "            input_var=inputvar, name='input')\n",
      "    l_last = l_input\n",
      "    to_invert=[]\n",
      "    specs=map(lambda s: s.split('-'), specstr.split(','))\n",
      "    layerIdx = 1\n",
      "    for spec in specs:\n",
      "        if len(spec) == 2 and spec[0] == 'd':\n",
      "            # do not append, because we don't do the inverse of a dropout\n",
      "            # layer on the way up\n",
      "            l_last = nn.layers.DropoutLayer(l_last, p=float(spec[1]), rescale=True,\n",
      "                    name='dropout{}'.format(layerIdx))\n",
      "        elif len(spec) == 2:\n",
      "            nfilt = int(spec[0])\n",
      "            fsize = int(spec[1])\n",
      "            if pInit:\n",
      "                print 'wInit will be {}'.format(pInit[0].shape)\n",
      "            wInit = pInit.pop(0) if pInit else nn.init.GlorotUniform()\n",
      "            l_last = nn.layers.Conv2DLayer(l_last, num_filters=nfilt,\n",
      "                    filter_size=(fsize,fsize),\n",
      "                    W=wInit,\n",
      "                    nonlinearity=None,\n",
      "                    b=None,\n",
      "                    name='conv{}'.format(layerIdx))\n",
      "            to_invert.append(l_last)\n",
      "            if pInit:\n",
      "                print 'bInit will be {}'.format(pInit[0].shape)\n",
      "            bInit = pInit.pop(0) if pInit else nn.init.Constant(0)\n",
      "            l_last = nn.layers.BiasLayer(l_last,\n",
      "                    b=bInit,\n",
      "                    name='bias{}'.format(layerIdx))\n",
      "            l_last = nn.layers.NonlinearityLayer(l_last,\n",
      "                    nonlinearity=nn.nonlinearities.tanh,\n",
      "                    name='nl{}'.format(layerIdx))\n",
      "        elif len(spec) == 1:\n",
      "            if pInit:\n",
      "                print 'wInit will be {}'.format(pInit[0].shape)\n",
      "            wInit = pInit.pop(0) if pInit else nn.init.GlorotUniform()\n",
      "            l_last = nn.layers.DenseLayer(l_last, num_units=int(spec[0]),\n",
      "                    W = nn.init.GlorotUniform(),\n",
      "                    nonlinearity=None,\n",
      "                    b=None,\n",
      "                    name='dense{}'.format(layerIdx))\n",
      "            to_invert.append(l_last)\n",
      "            if pInit:\n",
      "                print 'bInit will be {}'.format(pInit[0].shape)\n",
      "            bInit = pInit.pop(0) if pInit else nn.init.Constant(0)\n",
      "            l_last = nn.layers.BiasLayer(l_last,\n",
      "                    b=bInit,\n",
      "                    name='bias{}'.format(layerIdx))\n",
      "            l_last = nn.layers.NonlinearityLayer(l_last,\n",
      "                    nonlinearity=nn.nonlinearities.tanh,\n",
      "                    name='nl{}'.format(layerIdx))\n",
      "        layerIdx += 1\n",
      "    l_last.nonlinearity=nn.nonlinearities.linear\n",
      "    l_last.name='encode'\n",
      "    for lay in to_invert[::-1]:\n",
      "        l_last=nn.layers.InverseLayer(l_last, lay, name='inv_{}'.format(lay.name))\n",
      "        l_last = nn.layers.BiasLayer(l_last,\n",
      "                name='inv_bias_{}'.format(lay.name))\n",
      "        l_last = nn.layers.NonlinearityLayer(l_last,\n",
      "                nonlinearity=nn.nonlinearities.tanh,\n",
      "                name='inv_nl_{}'.format(lay.name))\n",
      "    l_output = nn.layers.ReshapeLayer(l_last, shape=(([0], -1)), name='output')\n",
      "    return l_output\n",
      "\n",
      "def get_picture_array(X, index, shp=(100,100),channels=3):\n",
      "    ret = ((X[index]+0.5)*255.).reshape(channels,shp[0],shp[1]) \\\n",
      "        .transpose(2,1,0).clip(0,255).astype(np.uint8)\n",
      "    if channels == 1:\n",
      "        ret=ret.reshape(shp[1],shp[0])\n",
      "    return ret\n",
      "\n",
      "def get_image_pair(X, Xpr,channels=3,idx=-1):\n",
      "    mode = 'RGB' if channels == 3 else 'L'\n",
      "    index = prng.randint(X.shape[0]) if idx == -1 else idx\n",
      "    shp=X[0][0].shape\n",
      "    original_image = Image.fromarray(get_picture_array(X, index,shp,channels),mode=mode)\n",
      "    new_size = (original_image.size[0], original_image.size[1]*2)\n",
      "    new_im = Image.new(mode, new_size)\n",
      "    new_im.paste(original_image, (0,0))\n",
      "    rec_image = Image.fromarray(get_picture_array(Xpr, index,shp,channels),mode=mode)\n",
      "    new_im.paste(rec_image, (0,original_image.size[1]))\n",
      "    return new_im\n",
      "\n",
      "def crop_all(X, shrinkby = 0.1, shift = [0.5,0.5]):\n",
      "    ch = X.shape[1]\n",
      "    w,h = X.shape[2], X.shape[3]\n",
      "    shr = np.multiply(shrinkby, [w, h]).astype(np.int)\n",
      "    shift = np.multiply(shr, shift).astype(np.int)\n",
      "    sel = X[:,:,shift[0]:w-shr[0]+shift[0],shift[1]:h-shr[1]+shift[1]]\n",
      "    ret = []\n",
      "    mode = 'RGB' if ch == 3 else 'L'\n",
      "    for i in range(sel.shape[0]):\n",
      "        im = Image.fromarray(get_picture_array(sel, i, shp=(w-shr[0], h-shr[1]), channels=ch),\n",
      "                mode=mode)\n",
      "        im = im.resize((w,h), Image.ANTIALIAS)\n",
      "        ret.append(np.asarray(im.getdata()).reshape(h,w,ch).transpose(2,1,0))\n",
      "    ret = np.vstack(ret).astype(np.float32).reshape(-1, ch, w, h) / 255. - 0.5\n",
      "    return ret\n",
      "\n",
      "def iterator(X, batchsize, shuffle=False, flip=False):\n",
      "    indices = np.arange(len(X))\n",
      "    if shuffle:\n",
      "        prng.shuffle(indices)\n",
      "    for i in range(0, len(X) - batchsize + 1, batchsize):\n",
      "        sli = indices[i:i+batchsize]\n",
      "        yield X[sli]\n",
      "\n",
      "def load_params(model, fn):\n",
      "    with open(fn, 'r') as re:\n",
      "        nn.layers.set_all_param_values(model, pickle.load(re))\n",
      "\n",
      "def save_params(model, fn):\n",
      "    with open(fn, 'w') as wr:\n",
      "        pickle.dump(nn.layers.get_all_param_values(model), wr)\n",
      "\n",
      "# reset shared variable values of accumulators to recover from\n",
      "# an infinite loss. \n",
      "def reset_accs(updates, params):\n",
      "    for key in updates:\n",
      "        if not key in params:\n",
      "            v = key.get_value(borrow=True)\n",
      "            key.set_value(np.zeros(v.shape,dtype=v.dtype))\n",
      "\n",
      "def main(specstr='', z_units=512):\n",
      "    inp1 = T.tensor4('inputs')\n",
      "    gray=False\n",
      "    ch=1 if gray else 3\n",
      "    network = build_cae(inp1, imgshape=(300, 126),channels=ch,\n",
      "            specstr=specstr.format(z_units))\n",
      "    laylist = nn.layers.get_all_layers(network)\n",
      "    for l in laylist:\n",
      "        print l.name, nn.layers.get_output_shape(l)\n",
      "    enc_layer = laylist[next(i for i in xrange(len(laylist)) if laylist[i].name=='encode')]\n",
      "    pred = nn.layers.get_output(network)\n",
      "    z = nn.layers.get_output(enc_layer)\n",
      "    loss = nn.objectives.squared_error(pred, inp1.flatten(2)).mean()\n",
      "    params = nn.layers.get_all_params(network, trainable=True)\n",
      "    grads = nn.updates.total_norm_constraint(T.grad(loss, params), 10)\n",
      "    updates = nn.updates.adadelta(grads, params)\n",
      "    te_pred = nn.layers.get_output(network, deterministic=True)\n",
      "    te_loss = nn.objectives.squared_error(te_pred, inp1.flatten(2)).mean()\n",
      "    print 'compiling functions'\n",
      "    train_fn = theano.function([inp1], loss, updates=updates)\n",
      "    val_fn = theano.function([inp1], te_loss)\n",
      "    hist=[]\n",
      "    num_epochs = 30\n",
      "    load_epo = 0\n",
      "    min_err = -1\n",
      "    outpath=sys.argv[2]\n",
      "    best_params_file=os.path.join(outpath, 'init.params')\n",
      "    best_params = nn.layers.get_all_param_values(network)\n",
      "    print 'loading data'\n",
      "    X_tr, _ = load(sys.argv[1], split=1.,gray=gray,limit=256)\n",
      "    X_val, _ = load(sys.argv[1], split=1., gray=gray,limit=512)\n",
      "    val_err = 0\n",
      "    min_val_err = 100\n",
      "    if len(sys.argv) > 3:\n",
      "        print 'loading params from {}'.format(sys.argv[3])\n",
      "        load_params(network, sys.argv[3])\n",
      "    mveparams = nn.layers.get_all_param_values(network)\n",
      "    for epoch in range(num_epochs):\n",
      "        nsave=0\n",
      "        extested = 0\n",
      "        start = time.time()\n",
      "        if load_epo >= 4:\n",
      "            print 'reloading data'\n",
      "            X_tr, X_val = None, None\n",
      "            X_tr, _ = load(sys.argv[1], split=1.,gray=gray,limit=4096)\n",
      "            X_val, _ = load(sys.argv[1], split=1., gray=gray,limit=512)\n",
      "            load_epo=0\n",
      "        load_epo += 1\n",
      "        tr_err=0\n",
      "        tr_batches=0\n",
      "        for batch in iterator(X_tr, 8, shuffle=True):\n",
      "            extested += batch.shape[0]\n",
      "            tr_err += train_fn(batch)\n",
      "            tr_batches += 1\n",
      "            if math.isnan(tr_err):\n",
      "                print 'explosion, rev to {}'\n",
      "                tr_err = 0\n",
      "                tr_batches = 0\n",
      "                nn.layers.set_all_param_values(network,best_params)\n",
      "                reset_accs(updates, params)\n",
      "                load_epo -=1\n",
      "            elif extested % 100 == 0:\n",
      "                val_err=0\n",
      "                val_batches=0\n",
      "                for batch in iterator(X_val, 4, shuffle=False):\n",
      "                    val_err += val_fn(batch)\n",
      "                    val_batches += 1\n",
      "                    if math.isnan(val_err):\n",
      "                        break\n",
      "                val_err /= (val_batches if val_batches > 0 else 1)\n",
      "                print 'ep {:.3f}/{} - tl {:.5f} - vl {:.5f} - mve {:.5f} - t {:.3f}s'.format(\n",
      "                        epoch+(extested*1./X_tr.shape[0]), num_epochs,\n",
      "                        tr_err/tr_batches, val_err, min_val_err, time.time()-start)\n",
      "                hist.append([tr_err/tr_batches, val_err])\n",
      "                tr_err=0\n",
      "                tr_batches=0\n",
      "                start=time.time()\n",
      "                nsave += 1\n",
      "                if min_err==-1 or val_err<.9*min_err or (nsave>=6 and val_err<1.2*min_err):\n",
      "                    nsave = 0\n",
      "                    min_err=val_err\n",
      "                    oldfile=best_params_file\n",
      "                    best_params_file=os.path.join(outpath,\n",
      "                            'prms_{}_{:3f}.params'.format(epoch, val_err))\n",
      "                    best_params=nn.layers.get_all_param_values(network)\n",
      "                    print 'saving to {}'.format(best_params_file)\n",
      "                    #save_params(network, best_params_file)\n",
      "                    if os.path.isfile(oldfile):\n",
      "                        os.remove(oldfile)\n",
      "                if val_err < min_val_err:\n",
      "                    min_val_err = val_err\n",
      "                    mveparams=nn.layers.get_all_param_values(network)\n",
      "        pr = []\n",
      "        tg = []\n",
      "        for batch in iterator(X_val, 8, shuffle=False):\n",
      "            pr.append(nn.layers.get_output(network, batch, deterministic=True)\\\n",
      "                .eval().reshape(-1,ch,300,126))\n",
      "            tg.append(batch)\n",
      "            if len(pr) * 8 > 100:\n",
      "                break\n",
      "        pr = np.vstack(pr).reshape(-1, ch, 300, 126)\n",
      "        tg = np.vstack(tg).reshape(-1, ch, 300, 126)\n",
      "        for i in range(pr.shape[0]):\n",
      "            get_image_pair(tg, pr,channels=ch,idx=i).save('output_{}.jpg'.format(i))\n",
      "    nn.layers.set_all_param_values(network, mveparams)\n",
      "    save_params(network, 'params/final_{}.params'.format(min_val_err))\n",
      "    hist = np.asarray(hist)\n",
      "    np.savetxt('train_hist.csv', np.asarray(hist), delimiter=',')\n",
      "    plt.plot(hist[:,0], label='train')\n",
      "    plt.plot(hist[:,1], label='valid')\n",
      "    plt.yscale('log')\n",
      "    plt.savefig('train_hist.png')\n",
      "\n",
      "def encoder_decoder(paramsfile, channels=3, specstr=''):\n",
      "    inp = T.tensor4('inputs')\n",
      "    network = build_cae(inp, imgshape=(300, 126),channels=channels,specstr=specstr)\n",
      "    load_params(network, paramsfile)\n",
      "    laylist = nn.layers.get_all_layers(network)\n",
      "    enc_layer_idx = next(i for i in xrange(len(laylist)) if laylist[i].name=='encode')\n",
      "    enc_layer = laylist[enc_layer_idx]\n",
      "    return (lambda x: nn.layers.get_output(enc_layer, inputs=x,deterministic=True).eval(),\n",
      "            lambda x: nn.layers.get_output(network,\n",
      "                inputs={laylist[0]:np.zeros((x.shape[0],channels,300,126),\n",
      "                    dtype=theano.config.floatX),\n",
      "                    enc_layer:x}).eval().reshape(-1,channels,300,126))\n",
      "\n",
      "def decode_file(codefile, paramsfile, channels=3, specstr='', z_units=512):\n",
      "    encode,decode = encoder_decoder(paramsfile, channels=channels,\n",
      "            specstr=specstr.format(z_units))\n",
      "    codes = read_csv(codefile, header=False).values\n",
      "    imgs = decode(codes)\n",
      "    for i in xrange(imgs.shape[0]):\n",
      "        im = Image.fromarray(get_picture_array(imgs, i, shp=(300,126), channels=3),\n",
      "                mode = 'RGB' if channels==3 else 'L')\n",
      "        im.save('imgs/rec_{}.jpg'.format(i))\n",
      "\n",
      "def encode_dir(datadir, paramsfile,channels=3,specstr='',z_units=512):\n",
      "    encode, decode = encoder_decoder(paramsfile, channels=channels,\n",
      "            specstr=specstr.format(z_units))\n",
      "    codes = []\n",
      "    while True:\n",
      "        X, _ = load(datadir, split=1., gray=(channels==1), limit=300, randomize=True)\n",
      "        if X.shape[0] == 0:\n",
      "            break\n",
      "        #X = crop_all(X, shrinkby=0.1)\n",
      "        codes.append(enacode(X))\n",
      "    np.savetxt('codes.csv', codes, delimiter=',', fmt='%.5f')\n",
      "\n",
      "def test(datadir, paramsfile, channels=3,specstr=''):\n",
      "    inp = T.tensor4('inputs')\n",
      "    X, _ = load(datadir, split=1., gray=(channels==1), limit=50, randomize=False)\n",
      "    w,h=X.shape[2],X.shape[3]\n",
      "    network = build_cae(inp, imgshape=(w,h),channels=channels,specstr=specstr)\n",
      "    load_params(network, paramsfile)\n",
      "    pr = []\n",
      "    for batch in iterator(X, 10, shuffle=False):\n",
      "        pr.append(nn.layers.get_output(network, batch).eval().reshape(-1,channels,w,h))\n",
      "    pr = np.vstack(pr).reshape(-1, channels, w, h)\n",
      "    for i in range(pr.shape[0]):\n",
      "        get_image_pair(X, pr,channels=channels,idx=i).save('film/frame_{}.jpg'.format(i))\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    #run16\n",
      "    #specstr='d-0.2,8-3,d-0.2,8-2,d-0.2,8-2,d-0.2,{}'\n",
      "    #z_units=512\n",
      "    specstr='d-0.2,8-3,d-0.2,16-3,d-0.2,16-3,d-0.2,{}'\n",
      "    z_units=128\n",
      "    main(specstr=specstr, z_units=z_units)\n",
      "    #encode_dir(sys.argv[1], sys.argv[2], specstr=specstr, z_units=z_units)\n",
      "    #test(sys.argv[1], sys.argv[2],specstr=specstr.format(z_units))\n",
      "    #decode_file('codes.csv', sys.argv[2], channels=3,\n",
      "    #        specstr=specstr, z_units=z_units)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}