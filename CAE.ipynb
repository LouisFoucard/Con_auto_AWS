{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sys\n",
      "import os\n",
      "import time\n",
      "\n",
      "import numpy as np\n",
      "import theano\n",
      "import theano.tensor as T\n",
      "\n",
      "import lasagne\n",
      "\n",
      "from lasagne.layers import get_output, InputLayer, DenseLayer, Upscale2DLayer, ReshapeLayer\n",
      "from lasagne.nonlinearities import rectify, leaky_rectify, tanh\n",
      "from lasagne.updates import nesterov_momentum\n",
      "from lasagne.objectives import categorical_crossentropy\n",
      "from nolearn.lasagne import NeuralNet, BatchIterator, PrintLayerInfo\n",
      "\n",
      "from lasagne.layers import Conv2DLayer as Conv2DLayer\n",
      "from lasagne.layers import MaxPool2DLayer as MaxPool2DLayer"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "Using gpu device 0: GRID K520 (CNMeM is disabled)\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def load_dataset():\n",
      "    # We first define a download function, supporting both Python 2 and 3.                                                                                                                                  \n",
      "    if sys.version_info[0] == 2:\n",
      "        from urllib import urlretrieve\n",
      "    else:\n",
      "        from urllib.request import urlretrieve\n",
      "\n",
      "    def download(filename, source='http://yann.lecun.com/exdb/mnist/'):\n",
      "        print(\"Downloading %s\" % filename)\n",
      "        urlretrieve(source + filename, filename)\n",
      "\n",
      "    # We then define functions for loading MNIST images and labels.                                                                                                                                         \n",
      "    # For convenience, they also download the requested files if needed.                                                                                                                                    \n",
      "    import gzip\n",
      "\n",
      "    def load_mnist_images(filename):\n",
      "        if not os.path.exists(filename):\n",
      "            download(filename)\n",
      "        # Read the inputs in Yann LeCun's binary format.                                                                                                                                                    \n",
      "        with gzip.open(filename, 'rb') as f:\n",
      "            data = np.frombuffer(f.read(), np.uint8, offset=16)\n",
      "        # The inputs are vectors now, we reshape them to monochrome 2D images,                                                                                                                              \n",
      "        # following the shape convention: (examples, channels, rows, columns)                                                                                                                               \n",
      "        data = data.reshape(-1, 1, 28, 28)\n",
      "        # The inputs come as bytes, we convert them to float32 in range [0,1].                                                                                                                              \n",
      "        # (Actually to range [0, 255/256], for compatibility to the version                                                                                                                                 \n",
      "        # provided at http://deeplearning.net/data/mnist/mnist.pkl.gz.)                                                                                                                                     \n",
      "        return data / np.float32(256)\n",
      "\n",
      "    def load_mnist_labels(filename):\n",
      "        if not os.path.exists(filename):\n",
      "            download(filename)\n",
      "        # Read the labels in Yann LeCun's binary format.                                                                                                                                                    \n",
      "        with gzip.open(filename, 'rb') as f:\n",
      "            data = np.frombuffer(f.read(), np.uint8, offset=8)\n",
      "        # The labels are vectors of integers now, that's exactly what we want.                                                                                                                              \n",
      "        return data\n",
      "\n",
      "    # We can now download and read the training and test set images and labels.                                                                                                                             \n",
      "    X_train = load_mnist_images('train-images-idx3-ubyte.gz')\n",
      "    y_train = load_mnist_labels('train-labels-idx1-ubyte.gz')\n",
      "    X_test = load_mnist_images('t10k-images-idx3-ubyte.gz')\n",
      "    y_test = load_mnist_labels('t10k-labels-idx1-ubyte.gz')\n",
      "\n",
      "    # We reserve the last 10000 training examples for validation.                                                                                                                                           \n",
      "    X_train, X_val = X_train[:-10000], X_train[-10000:]\n",
      "    y_train, y_val = y_train[:-10000], y_train[-10000:]\n",
      "\n",
      "    # We just return all the arrays in order, as expected in main().                                                                                                                                        \n",
      "    # (It doesn't matter how we do this as long as we can read them again.)                                                                                                                                 \n",
      "    return X_train, y_train, X_val, y_val, X_test, y_test\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X_train, y_train, X_val, y_val, X_test, y_test = load_dataset()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def build_cnn(input_var=None):\n",
      "    \n",
      "    conv_num_filters = 16\n",
      "    filter_size = 3\n",
      "    pool_size = 2\n",
      "    encode_size = 16\n",
      "    dense_mid_size = 128\n",
      "    pad_in = 'valid'\n",
      "    pad_out = 'full'\n",
      "\n",
      "    # Input layer, as usual:                                                                                                                                                                                \n",
      "    network = InputLayer(shape=(None, 1, 28, 28),\n",
      "                                        input_var=input_var,name=\"InputLayer_0\")                                                                                                                                   \n",
      "    network = Conv2DLayer(\n",
      "            network, num_filters=conv_num_filters, filter_size=(filter_size, filter_size),pad=pad_in,name = \"Conv2DLayer_1\")\n",
      "    \n",
      "    network = Conv2DLayer(\n",
      "            network, num_filters=conv_num_filters, filter_size=(filter_size, filter_size),pad=pad_in,name = \"Conv2DLayer_2\")\n",
      "                                                                                                                                        \n",
      "    network = MaxPool2DLayer(network, pool_size=(pool_size, pool_size),name = \"MaxPool2DLayer_3\")\n",
      "                                                                                                                                     \n",
      "    network = Conv2DLayer(\n",
      "            network, num_filters=2*conv_num_filters, filter_size=(filter_size, filter_size),pad=pad_in,name = \"Conv2DLayer_4\")\n",
      "\n",
      "    network = MaxPool2DLayer(network, pool_size=(pool_size, pool_size),name = \"MaxPool2DLayer_5\")\n",
      "\n",
      "    network = ReshapeLayer(network, shape=(([0], -1)),name = \"ReshapeLayer_6\")\n",
      "    \n",
      "    network = DenseLayer(network,num_units=dense_mid_size,name = \"dense_mid_7\")\n",
      "    \n",
      "    network = DenseLayer(network,num_units=encode_size,name = \"encode\")\n",
      "    \n",
      "    network = DenseLayer(network,num_units=dense_mid_size,name = \"dense_mid_9\")\n",
      "    \n",
      "    network = DenseLayer(network,num_units=800,name = \"dense_mid_10\")\n",
      "    \n",
      "    network = ReshapeLayer(network, shape = (([0], 2*conv_num_filters, 5, 5)),name = \"ReshapeLayer_11\")\n",
      "    \n",
      "    network = Upscale2DLayer(network, scale_factor=(pool_size, pool_size),name = \"Upscale2DLayer_12\")\n",
      "    \n",
      "    network = Conv2DLayer(\n",
      "            network, num_filters=2*conv_num_filters, filter_size=(filter_size, filter_size),pad=pad_out,name = \"Conv2DLayer_13\")\n",
      "    \n",
      "    network = Upscale2DLayer(network, scale_factor=(pool_size, pool_size),name = \"Upscale2DLayer_14\")\n",
      "    \n",
      "    network = Conv2DLayer(\n",
      "            network, num_filters=conv_num_filters, filter_size=(filter_size, filter_size),pad=pad_out,name = \"Conv2DLayer_15\")\n",
      "    \n",
      "    network = Conv2DLayer(\n",
      "            network, num_filters=conv_num_filters, filter_size=(filter_size, filter_size),pad=pad_out, name = \"Conv2DLayer_16\")\n",
      "    \n",
      "    network = ReshapeLayer(network, shape=(([0], -1)),name = \"ReshapeLayer_17\")\n",
      "\n",
      "    return network\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def iterate_minibatches(inputs, targets, batchsize, shuffle=False):\n",
      "    assert len(inputs) == len(targets)\n",
      "    if shuffle:\n",
      "        indices = np.arange(len(inputs))\n",
      "        np.random.shuffle(indices)\n",
      "    for start_idx in range(0, len(inputs) - batchsize + 1, batchsize):\n",
      "        if shuffle:\n",
      "            excerpt = indices[start_idx:start_idx + batchsize]\n",
      "        else:\n",
      "            excerpt = slice(start_idx, start_idx + batchsize)\n",
      "        yield inputs[excerpt], targets[excerpt]\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def main(model='cnn', num_epochs=500):\n",
      "    # Load the dataset                                                                                                                                                                                      \n",
      "    print(\"Loading data...\")\n",
      "    X_train, y_train, X_val, y_val, X_test, y_test = load_dataset()\n",
      "\n",
      "    # Prepare Theano variables for inputs and targets                                                                                                                                                       \n",
      "    input_var = T.tensor4('inputs')\n",
      "    target_var = T.ivector('targets')\n",
      "\n",
      "    # Create neural network model (depending on first command line parameter)                                                                                                                               \n",
      "    print(\"Building model and compiling functions...\")\n",
      "\n",
      "    network = build_cnn(input_var)\n",
      "    laylist = lasagne.layers.get_all_layers(network)\n",
      "    \n",
      "    for l in laylist:\n",
      "        print l.name, lasagne.layers.get_output_shape(l)\n",
      "        \n",
      "    # Create a loss expression for training, i.e., a scalar objective we want                                                                                                                               \n",
      "    # to minimize (for our multi-class problem, it is the cross-entropy loss):                                                                                                                              \n",
      "    prediction = lasagne.layers.get_output(network)\n",
      "    loss = lasagne.objectives.categorical_crossentropy(prediction, input_var.flatten(2))\n",
      "    loss = loss.mean()\n",
      "    # We could add some weight decay as well here, see lasagne.regularization.                                                                                                                              \n",
      "\n",
      "    # Create update expressions for training, i.e., how to modify the                                                                                                                                       \n",
      "    # parameters at each training step. Here, we'll use Stochastic Gradient                                                                                                                                 \n",
      "    # Descent (SGD) with Nesterov momentum, but Lasagne offers plenty more.                                                                                                                                 \n",
      "    params = lasagne.layers.get_all_params(network, trainable=True)\n",
      "    updates = lasagne.updates.nesterov_momentum(\n",
      "            loss, params, learning_rate=0.01, momentum=0.9)\n",
      "\n",
      "    # Create a loss expression for validation/testing. The crucial difference                                                                                                                               \n",
      "    # here is that we do a deterministic forward pass through the network,                                                                                                                                  \n",
      "    # disabling dropout layers.                                                                                                                                                                             \n",
      "    test_prediction = lasagne.layers.get_output(network, deterministic=True)\n",
      "    test_loss = lasagne.objectives.categorical_crossentropy(test_prediction,\n",
      "                                                            target_var)\n",
      "    test_loss = test_loss.mean()\n",
      "    # As a bonus, also create an expression for the classification accuracy:                                                                                                                                \n",
      "    test_acc = T.mean(T.eq(T.argmax(test_prediction, axis=1), target_var),\n",
      "                      dtype=theano.config.floatX)\n",
      "\n",
      "    # Compile a function performing a training step on a mini-batch (by giving                                                                                                                              \n",
      "    # the updates dictionary) and returning the corresponding training loss:                                                                                                                                \n",
      "    train_fn = theano.function([input_var, target_var], loss, updates=updates)\n",
      "\n",
      "    # Compile a second function computing the validation loss and accuracy:                                                                                                                                 \n",
      "    val_fn = theano.function([input_var, target_var], [test_loss, test_acc])\n",
      "\n",
      "    # Finally, launch the training loop.                                                                                                                                                                    \n",
      "    print(\"Starting training...\")\n",
      "    # We iterate over epochs:                                                                                                                                                                               \n",
      "    for epoch in range(num_epochs):\n",
      "        # In each epoch, we do a full pass over the training data:                                                                                                                                          \n",
      "        train_err = 0\n",
      "        train_batches = 0\n",
      "        start_time = time.time()\n",
      "        for batch in iterate_minibatches(X_train, y_train, 500, shuffle=True):\n",
      "            inputs, targets = batch\n",
      "            train_err += train_fn(inputs, targets)\n",
      "            train_batches += 1\n",
      "\n",
      "        # And a full pass over the validation data:                                                                                                                                                         \n",
      "        val_err = 0\n",
      "        val_acc = 0\n",
      "        val_batches = 0\n",
      "        for batch in iterate_minibatches(X_val, y_val, 500, shuffle=False):\n",
      "            inputs, targets = batch\n",
      "            err, acc = val_fn(inputs, targets)\n",
      "            val_err += err\n",
      "            val_acc += acc\n",
      "            val_batches += 1\n",
      "\n",
      "        # Then we print the results for this epoch:                                                                                                                                                         \n",
      "        print(\"Epoch {} of {} took {:.3f}s\".format(\n",
      "            epoch + 1, num_epochs, time.time() - start_time))\n",
      "        print(\"  training loss:\\t\\t{:.6f}\".format(train_err / train_batches))\n",
      "        print(\"  validation loss:\\t\\t{:.6f}\".format(val_err / val_batches))\n",
      "        print(\"  validation accuracy:\\t\\t{:.2f} %\".format(\n",
      "            val_acc / val_batches * 100))\n",
      "        # After training, we compute and print the test error:                                                                                                                                                  \n",
      "    test_err = 0\n",
      "    test_acc = 0\n",
      "    test_batches = 0\n",
      "    for batch in iterate_minibatches(X_test, y_test, 500, shuffle=False):\n",
      "        inputs, targets = batch\n",
      "        err, acc = val_fn(inputs, targets)\n",
      "        test_err += err\n",
      "        test_acc += acc\n",
      "        test_batches += 1\n",
      "    print(\"Final results:\")\n",
      "    print(\"  test loss:\\t\\t\\t{:.6f}\".format(test_err / test_batches))\n",
      "    print(\"  test accuracy:\\t\\t{:.2f} %\".format(\n",
      "        test_acc / test_batches * 100))\n",
      "\n",
      "    # Optionally, you could now dump the network weights to a file like this:                                                                                                                               \n",
      "    # np.savez('model.npz', *lasagne.layers.get_all_param_values(network))                                                                                                                                  \n",
      "    #                                                                                                                                                                                                       \n",
      "    # And load them again later on like this:                                                                                                                                                               \n",
      "    # with np.load('model.npz') as f:                                                                                                                                                                       \n",
      "    #     param_values = [f['arr_%d' % i] for i in range(len(f.files))]                                                                                                                                     \n",
      "    # lasagne.layers.set_all_param_values(network, param_values)  \n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "main()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Loading data...\n",
        "Building model and compiling functions..."
       ]
      },
      {
       "ename": "UnusedInputError",
       "evalue": "theano.function was asked to create a function computing outputs given certain inputs, but the provided input variable at index 1 is not part of the computational graph needed to compute the outputs: targets.\nTo make this error into a warning, you can pass the parameter on_unused_input='warn' to theano.function. To disable it completely, use on_unused_input='ignore'.",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mUnusedInputError\u001b[0m                          Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-7-58ca95c5b364>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[0;32m<ipython-input-6-d99a7b661823>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(model, num_epochs)\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;31m# Compile a function performing a training step on a mini-batch (by giving\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;31m# the updates dictionary) and returning the corresponding training loss:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m     \u001b[0mtrain_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_var\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mupdates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;31m# Compile a second function computing the validation loss and accuracy:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/theano/compile/function.pyc\u001b[0m in \u001b[0;36mfunction\u001b[0;34m(inputs, outputs, mode, updates, givens, no_default_updates, accept_inplace, name, rebuild_strict, allow_input_downcast, profile, on_unused_input)\u001b[0m\n\u001b[1;32m    306\u001b[0m                    \u001b[0mon_unused_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mon_unused_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m                    \u001b[0mprofile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 308\u001b[0;31m                    output_keys=output_keys)\n\u001b[0m\u001b[1;32m    309\u001b[0m     \u001b[0;31m# We need to add the flag check_aliased inputs if we have any mutable or\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m     \u001b[0;31m# borrowed used defined inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/theano/compile/pfunc.pyc\u001b[0m in \u001b[0;36mpfunc\u001b[0;34m(params, outputs, mode, updates, givens, no_default_updates, accept_inplace, name, rebuild_strict, allow_input_downcast, profile, on_unused_input, output_keys)\u001b[0m\n\u001b[1;32m    524\u001b[0m                          \u001b[0maccept_inplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maccept_inplace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m                          \u001b[0mprofile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon_unused_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mon_unused_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 526\u001b[0;31m                          output_keys=output_keys)\n\u001b[0m\u001b[1;32m    527\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36morig_function\u001b[0;34m(inputs, outputs, mode, accept_inplace, name, profile, on_unused_input, output_keys)\u001b[0m\n\u001b[1;32m   1775\u001b[0m                    \u001b[0mprofile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1776\u001b[0m                    \u001b[0mon_unused_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mon_unused_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1777\u001b[0;31m                    \u001b[0moutput_keys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1778\u001b[0m             defaults)\n\u001b[1;32m   1779\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, inputs, outputs, mode, accept_inplace, function_builder, profile, on_unused_input, fgraph, output_keys)\u001b[0m\n\u001b[1;32m   1414\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1415\u001b[0m         \u001b[0;31m# Check if some input variables are unused\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1416\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_unused_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon_unused_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1417\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1418\u001b[0m         \u001b[0;31m# Make a list of (SymbolicInput|SymblicInputKits, indices,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m_check_unused_inputs\u001b[0;34m(self, inputs, outputs, on_unused_input)\u001b[0m\n\u001b[1;32m   1552\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mon_unused_input\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'raise'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1553\u001b[0m                     raise UnusedInputError(msg % (inputs.index(i),\n\u001b[0;32m-> 1554\u001b[0;31m                                                   i.variable, err_msg))\n\u001b[0m\u001b[1;32m   1555\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1556\u001b[0m                     raise ValueError(\"Invalid value for keyword \"\n",
        "\u001b[0;31mUnusedInputError\u001b[0m: theano.function was asked to create a function computing outputs given certain inputs, but the provided input variable at index 1 is not part of the computational graph needed to compute the outputs: targets.\nTo make this error into a warning, you can pass the parameter on_unused_input='warn' to theano.function. To disable it completely, use on_unused_input='ignore'."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "InputLayer_0 (None, 1, 28, 28)\n",
        "Conv2DLayer_1 (None, 16, 26, 26)\n",
        "Conv2DLayer_2 (None, 16, 24, 24)\n",
        "MaxPool2DLayer_3 (None, 16, 12, 12)\n",
        "Conv2DLayer_4 (None, 32, 10, 10)\n",
        "MaxPool2DLayer_5 (None, 32, 5, 5)\n",
        "ReshapeLayer_6 (None, 800)\n",
        "dense_mid_7 (None, 128)\n",
        "encode (None, 16)\n",
        "dense_mid_9 (None, 128)\n",
        "dense_mid_10 (None, 800)\n",
        "ReshapeLayer_11 (None, 32, 5, 5)\n",
        "Upscale2DLayer_12 (None, 32, 10, 10)\n",
        "Conv2DLayer_13 (None, 32, 12, 12)\n",
        "Upscale2DLayer_14 (None, 32, 24, 24)\n",
        "Conv2DLayer_15 (None, 16, 26, 26)\n",
        "Conv2DLayer_16 (None, 16, 28, 28)\n",
        "ReshapeLayer_17 (None, 12544)\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}