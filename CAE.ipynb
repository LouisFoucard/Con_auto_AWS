{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sys\n",
      "import os\n",
      "import time\n",
      "\n",
      "import numpy as np\n",
      "import theano\n",
      "import theano.tensor as T\n",
      "\n",
      "import lasagne\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "Using gpu device 0: GRID K520 (CNMeM is disabled)\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def load_dataset():\n",
      "    # We first define a download function, supporting both Python 2 and 3.                                                                                                                                  \n",
      "    if sys.version_info[0] == 2:\n",
      "        from urllib import urlretrieve\n",
      "    else:\n",
      "        from urllib.request import urlretrieve\n",
      "\n",
      "    def download(filename, source='http://yann.lecun.com/exdb/mnist/'):\n",
      "        print(\"Downloading %s\" % filename)\n",
      "        urlretrieve(source + filename, filename)\n",
      "\n",
      "    # We then define functions for loading MNIST images and labels.                                                                                                                                         \n",
      "    # For convenience, they also download the requested files if needed.                                                                                                                                    \n",
      "    import gzip\n",
      "\n",
      "    def load_mnist_images(filename):\n",
      "        if not os.path.exists(filename):\n",
      "            download(filename)\n",
      "        # Read the inputs in Yann LeCun's binary format.                                                                                                                                                    \n",
      "        with gzip.open(filename, 'rb') as f:\n",
      "            data = np.frombuffer(f.read(), np.uint8, offset=16)\n",
      "        # The inputs are vectors now, we reshape them to monochrome 2D images,                                                                                                                              \n",
      "        # following the shape convention: (examples, channels, rows, columns)                                                                                                                               \n",
      "        data = data.reshape(-1, 1, 28, 28)\n",
      "        # The inputs come as bytes, we convert them to float32 in range [0,1].                                                                                                                              \n",
      "        # (Actually to range [0, 255/256], for compatibility to the version                                                                                                                                 \n",
      "        # provided at http://deeplearning.net/data/mnist/mnist.pkl.gz.)                                                                                                                                     \n",
      "        return data / np.float32(256)\n",
      "\n",
      "    def load_mnist_labels(filename):\n",
      "        if not os.path.exists(filename):\n",
      "            download(filename)\n",
      "        # Read the labels in Yann LeCun's binary format.                                                                                                                                                    \n",
      "        with gzip.open(filename, 'rb') as f:\n",
      "            data = np.frombuffer(f.read(), np.uint8, offset=8)\n",
      "        # The labels are vectors of integers now, that's exactly what we want.                                                                                                                              \n",
      "        return data\n",
      "\n",
      "    # We can now download and read the training and test set images and labels.                                                                                                                             \n",
      "    X_train = load_mnist_images('train-images-idx3-ubyte.gz')\n",
      "    y_train = load_mnist_labels('train-labels-idx1-ubyte.gz')\n",
      "    X_test = load_mnist_images('t10k-images-idx3-ubyte.gz')\n",
      "    y_test = load_mnist_labels('t10k-labels-idx1-ubyte.gz')\n",
      "\n",
      "    # We reserve the last 10000 training examples for validation.                                                                                                                                           \n",
      "    X_train, X_val = X_train[:-10000], X_train[-10000:]\n",
      "    y_train, y_val = y_train[:-10000], y_train[-10000:]\n",
      "\n",
      "    # We just return all the arrays in order, as expected in main().                                                                                                                                        \n",
      "    # (It doesn't matter how we do this as long as we can read them again.)                                                                                                                                 \n",
      "    return X_train, y_train, X_val, y_val, X_test, y_test\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X_train, y_train, X_val, y_val, X_test, y_test = load_dataset()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Downloading train-images-idx3-ubyte.gz\n",
        "Downloading train-labels-idx1-ubyte.gz"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Downloading t10k-images-idx3-ubyte.gz"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Downloading t10k-labels-idx1-ubyte.gz"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def build_cnn(input_var=None):\n",
      "\n",
      "    # Input layer, as usual:                                                                                                                                                                                \n",
      "    network = lasagne.layers.InputLayer(shape=(None, 1, 28, 28),\n",
      "                                        input_var=input_var)\n",
      "    # This time we do not apply input dropout, as it tends to work less well                                                                                                                                \n",
      "    # for convolutional layers.                                                                                                                                                                             \n",
      "\n",
      "    # Convolutional layer with 32 kernels of size 5x5. Strided and padded                                                                                                                                   \n",
      "    # convolutions are supported as well; see the docstring.                                                                                                                                                \n",
      "    network = lasagne.layers.Conv2DLayer(\n",
      "            network, num_filters=32, filter_size=(5, 5),\n",
      "            nonlinearity=lasagne.nonlinearities.rectify,\n",
      "            W=lasagne.init.GlorotUniform())\n",
      "    # Expert note: Lasagne provides alternative convolutional layers that                                                                                                                                   \n",
      "    # override Theano's choice of which implementation to use; for details                                                                                                                                  \n",
      "    # please see http://lasagne.readthedocs.org/en/latest/user/tutorial.html.                                                                                                                               \n",
      "\n",
      "    # Max-pooling layer of factor 2 in both dimensions:                                                                                                                                                     \n",
      "    network = lasagne.layers.MaxPool2DLayer(network, pool_size=(2, 2))\n",
      "\n",
      "    # Another convolution with 32 5x5 kernels, and another 2x2 pooling:                                                                                                                                     \n",
      "    network = lasagne.layers.Conv2DLayer(\n",
      "            network, num_filters=32, filter_size=(5, 5),\n",
      "            nonlinearity=lasagne.nonlinearities.rectify)\n",
      "\n",
      "    network = lasagne.layers.MaxPool2DLayer(network, pool_size=(2, 2))\n",
      "\n",
      "    # A fully-connected layer of 256 units with 50% dropout on its inputs:                                                                                                                                  \n",
      "    network = lasagne.layers.DenseLayer(\n",
      "            lasagne.layers.dropout(network, p=.5),\n",
      "            num_units=256,\n",
      "            nonlinearity=lasagne.nonlinearities.rectify)\n",
      "    \n",
      "    # And, finally, the 10-unit output layer with 50% dropout on its inputs:                                                                                                                                \n",
      "    network = lasagne.layers.DenseLayer(\n",
      "            lasagne.layers.dropout(network, p=.5),\n",
      "            num_units=10,\n",
      "            nonlinearity=lasagne.nonlinearities.softmax)\n",
      "\n",
      "    return network"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def iterate_minibatches(inputs, targets, batchsize, shuffle=False):\n",
      "    assert len(inputs) == len(targets)\n",
      "    if shuffle:\n",
      "        indices = np.arange(len(inputs))\n",
      "        np.random.shuffle(indices)\n",
      "    for start_idx in range(0, len(inputs) - batchsize + 1, batchsize):\n",
      "        if shuffle:\n",
      "            excerpt = indices[start_idx:start_idx + batchsize]\n",
      "        else:\n",
      "            excerpt = slice(start_idx, start_idx + batchsize)\n",
      "        yield inputs[excerpt], targets[excerpt]\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def main(model='cnn', num_epochs=500):\n",
      "    # Load the dataset                                                                                                                                                                                      \n",
      "    print(\"Loading data...\")\n",
      "    X_train, y_train, X_val, y_val, X_test, y_test = load_dataset()\n",
      "\n",
      "    # Prepare Theano variables for inputs and targets                                                                                                                                                       \n",
      "    input_var = T.tensor4('inputs')\n",
      "    target_var = T.ivector('targets')\n",
      "\n",
      "    # Create neural network model (depending on first command line parameter)                                                                                                                               \n",
      "    print(\"Building model and compiling functions...\")\n",
      "\n",
      "    network = build_cnn(input_var)\n",
      "\n",
      "    # Create a loss expression for training, i.e., a scalar objective we want                                                                                                                               \n",
      "    # to minimize (for our multi-class problem, it is the cross-entropy loss):                                                                                                                              \n",
      "    prediction = lasagne.layers.get_output(network)\n",
      "    loss = lasagne.objectives.categorical_crossentropy(prediction, target_var)\n",
      "    loss = loss.mean()\n",
      "    # We could add some weight decay as well here, see lasagne.regularization.                                                                                                                              \n",
      "\n",
      "    # Create update expressions for training, i.e., how to modify the                                                                                                                                       \n",
      "    # parameters at each training step. Here, we'll use Stochastic Gradient                                                                                                                                 \n",
      "    # Descent (SGD) with Nesterov momentum, but Lasagne offers plenty more.                                                                                                                                 \n",
      "    params = lasagne.layers.get_all_params(network, trainable=True)\n",
      "    updates = lasagne.updates.nesterov_momentum(\n",
      "            loss, params, learning_rate=0.01, momentum=0.9)\n",
      "\n",
      "    # Create a loss expression for validation/testing. The crucial difference                                                                                                                               \n",
      "    # here is that we do a deterministic forward pass through the network,                                                                                                                                  \n",
      "    # disabling dropout layers.                                                                                                                                                                             \n",
      "    test_prediction = lasagne.layers.get_output(network, deterministic=True)\n",
      "    test_loss = lasagne.objectives.categorical_crossentropy(test_prediction,\n",
      "                                                            target_var)\n",
      "    test_loss = test_loss.mean()\n",
      "    # As a bonus, also create an expression for the classification accuracy:                                                                                                                                \n",
      "    test_acc = T.mean(T.eq(T.argmax(test_prediction, axis=1), target_var),\n",
      "                      dtype=theano.config.floatX)\n",
      "\n",
      "    # Compile a function performing a training step on a mini-batch (by giving                                                                                                                              \n",
      "    # the updates dictionary) and returning the corresponding training loss:                                                                                                                                \n",
      "    train_fn = theano.function([input_var, target_var], loss, updates=updates)\n",
      "\n",
      "    # Compile a second function computing the validation loss and accuracy:                                                                                                                                 \n",
      "    val_fn = theano.function([input_var, target_var], [test_loss, test_acc])\n",
      "\n",
      "    # Finally, launch the training loop.                                                                                                                                                                    \n",
      "    print(\"Starting training...\")\n",
      "    # We iterate over epochs:                                                                                                                                                                               \n",
      "    for epoch in range(num_epochs):\n",
      "        # In each epoch, we do a full pass over the training data:                                                                                                                                          \n",
      "        train_err = 0\n",
      "        train_batches = 0\n",
      "        start_time = time.time()\n",
      "        for batch in iterate_minibatches(X_train, y_train, 500, shuffle=True):\n",
      "            inputs, targets = batch\n",
      "            train_err += train_fn(inputs, targets)\n",
      "            train_batches += 1\n",
      "\n",
      "        # And a full pass over the validation data:                                                                                                                                                         \n",
      "        val_err = 0\n",
      "        val_acc = 0\n",
      "        val_batches = 0\n",
      "        for batch in iterate_minibatches(X_val, y_val, 500, shuffle=False):\n",
      "            inputs, targets = batch\n",
      "            err, acc = val_fn(inputs, targets)\n",
      "            val_err += err\n",
      "            val_acc += acc\n",
      "            val_batches += 1\n",
      "\n",
      "        # Then we print the results for this epoch:                                                                                                                                                         \n",
      "        print(\"Epoch {} of {} took {:.3f}s\".format(\n",
      "            epoch + 1, num_epochs, time.time() - start_time))\n",
      "        print(\"  training loss:\\t\\t{:.6f}\".format(train_err / train_batches))\n",
      "        print(\"  validation loss:\\t\\t{:.6f}\".format(val_err / val_batches))\n",
      "        print(\"  validation accuracy:\\t\\t{:.2f} %\".format(\n",
      "            val_acc / val_batches * 100))\n",
      "        # After training, we compute and print the test error:                                                                                                                                                  \n",
      "    test_err = 0\n",
      "    test_acc = 0\n",
      "    test_batches = 0\n",
      "    for batch in iterate_minibatches(X_test, y_test, 500, shuffle=False):\n",
      "        inputs, targets = batch\n",
      "        err, acc = val_fn(inputs, targets)\n",
      "        test_err += err\n",
      "        test_acc += acc\n",
      "        test_batches += 1\n",
      "    print(\"Final results:\")\n",
      "    print(\"  test loss:\\t\\t\\t{:.6f}\".format(test_err / test_batches))\n",
      "    print(\"  test accuracy:\\t\\t{:.2f} %\".format(\n",
      "        test_acc / test_batches * 100))\n",
      "\n",
      "    # Optionally, you could now dump the network weights to a file like this:                                                                                                                               \n",
      "    # np.savez('model.npz', *lasagne.layers.get_all_param_values(network))                                                                                                                                  \n",
      "    #                                                                                                                                                                                                       \n",
      "    # And load them again later on like this:                                                                                                                                                               \n",
      "    # with np.load('model.npz') as f:                                                                                                                                                                       \n",
      "    #     param_values = [f['arr_%d' % i] for i in range(len(f.files))]                                                                                                                                     \n",
      "    # lasagne.layers.set_all_param_values(network, param_values)  \n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "main()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Loading data...\n",
        "Building model and compiling functions..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Starting training..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Epoch 1 of 500 took 3.045s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "  training loss:\t\t1.600007\n",
        "  validation loss:\t\t0.338686\n",
        "  validation accuracy:\t\t91.04 %\n",
        "Epoch 2 of 500 took 3.003s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "  training loss:\t\t0.498065\n",
        "  validation loss:\t\t0.181136\n",
        "  validation accuracy:\t\t94.76 %\n",
        "Epoch 3 of 500 took 3.005s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "  training loss:\t\t0.347999\n",
        "  validation loss:\t\t0.138318\n",
        "  validation accuracy:\t\t96.13 %\n",
        "Epoch 4 of 500 took 3.001s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "  training loss:\t\t0.287125\n",
        "  validation loss:\t\t0.117833\n",
        "  validation accuracy:\t\t96.62 %\n",
        "Epoch 5 of 500 took 3.002s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "  training loss:\t\t0.252799\n",
        "  validation loss:\t\t0.105216\n",
        "  validation accuracy:\t\t96.96 %\n",
        "Epoch 6 of 500 took 3.002s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "  training loss:\t\t0.223718\n",
        "  validation loss:\t\t0.092353\n",
        "  validation accuracy:\t\t97.34 %\n",
        "Epoch 7 of 500 took 3.007s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "  training loss:\t\t0.196093\n",
        "  validation loss:\t\t0.081391\n",
        "  validation accuracy:\t\t97.78 %\n",
        "Epoch 8 of 500 took 3.008s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "  training loss:\t\t0.187548\n",
        "  validation loss:\t\t0.074552\n",
        "  validation accuracy:\t\t97.87 %\n",
        "Epoch 9 of 500 took 2.996s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "  training loss:\t\t0.172503\n",
        "  validation loss:\t\t0.072454\n",
        "  validation accuracy:\t\t97.96 %\n",
        "Epoch 10 of 500 took 3.001s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "  training loss:\t\t0.161089\n",
        "  validation loss:\t\t0.065014\n",
        "  validation accuracy:\t\t98.14 %\n",
        "Epoch 11 of 500 took 3.001s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "  training loss:\t\t0.156239\n",
        "  validation loss:\t\t0.062045\n",
        "  validation accuracy:\t\t98.25 %\n",
        "Epoch 12 of 500 took 3.001s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "  training loss:\t\t0.145951\n",
        "  validation loss:\t\t0.059798\n",
        "  validation accuracy:\t\t98.32 %\n",
        "Epoch 13 of 500 took 2.999s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "  training loss:\t\t0.140720\n",
        "  validation loss:\t\t0.057057\n",
        "  validation accuracy:\t\t98.40 %\n",
        "Epoch 14 of 500 took 2.996s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "  training loss:\t\t0.130648\n",
        "  validation loss:\t\t0.054229\n",
        "  validation accuracy:\t\t98.50 %\n",
        "Epoch 15 of 500 took 3.001s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "  training loss:\t\t0.126834\n",
        "  validation loss:\t\t0.052962\n",
        "  validation accuracy:\t\t98.53 %\n",
        "Epoch 16 of 500 took 3.003s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "  training loss:\t\t0.121230\n",
        "  validation loss:\t\t0.053578\n",
        "  validation accuracy:\t\t98.45 %\n",
        "Epoch 17 of 500 took 3.003s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "  training loss:\t\t0.117124\n",
        "  validation loss:\t\t0.051883\n",
        "  validation accuracy:\t\t98.51 %\n",
        "Epoch 18 of 500 took 3.004s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "  training loss:\t\t0.113208\n",
        "  validation loss:\t\t0.049914\n",
        "  validation accuracy:\t\t98.60 %\n",
        "Epoch 19 of 500 took 3.007s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "  training loss:\t\t0.109715\n",
        "  validation loss:\t\t0.048265\n",
        "  validation accuracy:\t\t98.64 %\n",
        "Epoch 20 of 500 took 3.002s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "  training loss:\t\t0.106464\n",
        "  validation loss:\t\t0.047141\n",
        "  validation accuracy:\t\t98.65 %\n",
        "Epoch 21 of 500 took 2.996s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "  training loss:\t\t0.104036\n",
        "  validation loss:\t\t0.046729\n",
        "  validation accuracy:\t\t98.67 %\n",
        "Epoch 22 of 500 took 3.003s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "  training loss:\t\t0.098767\n",
        "  validation loss:\t\t0.046178\n",
        "  validation accuracy:\t\t98.69 %\n",
        "Epoch 23 of 500 took 3.001s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "  training loss:\t\t0.097831\n",
        "  validation loss:\t\t0.043650\n",
        "  validation accuracy:\t\t98.78 %\n",
        "Epoch 24 of 500 took 3.005s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "  training loss:\t\t0.093444\n",
        "  validation loss:\t\t0.043381\n",
        "  validation accuracy:\t\t98.74 %\n",
        "Epoch 25 of 500 took 2.997s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "  training loss:\t\t0.091974\n",
        "  validation loss:\t\t0.043210\n",
        "  validation accuracy:\t\t98.74 %\n",
        "Epoch 26 of 500 took 3.021s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "  training loss:\t\t0.091080\n",
        "  validation loss:\t\t0.041320\n",
        "  validation accuracy:\t\t98.82 %\n",
        "Epoch 27 of 500 took 3.005s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "  training loss:\t\t0.088205\n",
        "  validation loss:\t\t0.041596\n",
        "  validation accuracy:\t\t98.79 %\n",
        "Epoch 28 of 500 took 3.002s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "  training loss:\t\t0.086599\n",
        "  validation loss:\t\t0.039989\n",
        "  validation accuracy:\t\t98.80 %\n",
        "Epoch 29 of 500 took 3.005s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "  training loss:\t\t0.083218\n",
        "  validation loss:\t\t0.040053\n",
        "  validation accuracy:\t\t98.86 %\n",
        "Epoch 30 of 500 took 2.997s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "  training loss:\t\t0.082783\n",
        "  validation loss:\t\t0.040034\n",
        "  validation accuracy:\t\t98.83 %\n",
        "Epoch 31 of 500 took 2.999s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "  training loss:\t\t0.081440\n",
        "  validation loss:\t\t0.039529\n",
        "  validation accuracy:\t\t98.86 %\n",
        "Epoch 32 of 500 took 3.000s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "  training loss:\t\t0.079500\n",
        "  validation loss:\t\t0.037838\n",
        "  validation accuracy:\t\t98.92 %\n",
        "Epoch 33 of 500 took 3.001s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "  training loss:\t\t0.079028\n",
        "  validation loss:\t\t0.039048\n",
        "  validation accuracy:\t\t98.86 %\n",
        "Epoch 34 of 500 took 3.000s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "  training loss:\t\t0.076854\n",
        "  validation loss:\t\t0.037808\n",
        "  validation accuracy:\t\t98.96 %\n",
        "Epoch 35 of 500 took 2.997s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "  training loss:\t\t0.074083\n",
        "  validation loss:\t\t0.036657\n",
        "  validation accuracy:\t\t98.91 %\n",
        "Epoch 36 of 500 took 3.002s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "  training loss:\t\t0.075054\n",
        "  validation loss:\t\t0.038979\n",
        "  validation accuracy:\t\t98.92 %\n",
        "Epoch 37 of 500 took 3.007s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "  training loss:\t\t0.071867\n",
        "  validation loss:\t\t0.036909\n",
        "  validation accuracy:\t\t98.91 %\n",
        "Epoch 38 of 500 took 2.993s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "  training loss:\t\t0.073592\n",
        "  validation loss:\t\t0.036230\n",
        "  validation accuracy:\t\t98.95 %\n",
        "Epoch 39 of 500 took 2.996s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "  training loss:\t\t0.069808\n",
        "  validation loss:\t\t0.035367\n",
        "  validation accuracy:\t\t99.00 %\n",
        "Epoch 40 of 500 took 3.000s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "  training loss:\t\t0.070052\n",
        "  validation loss:\t\t0.037253\n",
        "  validation accuracy:\t\t98.94 %\n",
        "Epoch 41 of 500 took 2.995s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "  training loss:\t\t0.067268\n",
        "  validation loss:\t\t0.036769\n",
        "  validation accuracy:\t\t98.94 %\n",
        "Epoch 42 of 500 took 3.002s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "  training loss:\t\t0.069737\n",
        "  validation loss:\t\t0.035289\n",
        "  validation accuracy:\t\t98.93 %\n",
        "Epoch 43 of 500 took 3.005s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "  training loss:\t\t0.066287\n",
        "  validation loss:\t\t0.035588\n",
        "  validation accuracy:\t\t98.93 %\n",
        "Epoch 44 of 500 took 2.997s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "  training loss:\t\t0.064504\n",
        "  validation loss:\t\t0.035046\n",
        "  validation accuracy:\t\t98.96 %\n",
        "Epoch 45 of 500 took 3.002s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "  training loss:\t\t0.065389\n",
        "  validation loss:\t\t0.034662\n",
        "  validation accuracy:\t\t99.05 %\n",
        "Epoch 46 of 500 took 3.001s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "  training loss:\t\t0.067052\n",
        "  validation loss:\t\t0.034935\n",
        "  validation accuracy:\t\t98.98 %\n",
        "Epoch 47 of 500 took 2.997s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "  training loss:\t\t0.064928\n",
        "  validation loss:\t\t0.035319\n",
        "  validation accuracy:\t\t98.96 %\n",
        "Epoch 48 of 500 took 2.997s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "  training loss:\t\t0.062457\n",
        "  validation loss:\t\t0.035069\n",
        "  validation accuracy:\t\t98.99 %\n",
        "Epoch 49 of 500 took 2.996s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "  training loss:\t\t0.062398\n",
        "  validation loss:\t\t0.033572\n",
        "  validation accuracy:\t\t99.07 %\n",
        "Epoch 50 of 500 took 2.997s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "  training loss:\t\t0.060616\n",
        "  validation loss:\t\t0.035625\n",
        "  validation accuracy:\t\t98.93 %\n",
        "Epoch 51 of 500 took 2.997s"
       ]
      }
     ],
     "prompt_number": "*"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}