{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sys\n",
      "import os\n",
      "import time\n",
      "\n",
      "import numpy as np\n",
      "import theano\n",
      "import theano.tensor as T\n",
      "\n",
      "import lasagne\n",
      "\n",
      "from lasagne.layers import get_output, InputLayer, DenseLayer, Upscale2DLayer, ReshapeLayer\n",
      "from lasagne.nonlinearities import rectify, leaky_rectify, tanh\n",
      "from lasagne.updates import nesterov_momentum\n",
      "from lasagne.objectives import categorical_crossentropy\n",
      "from nolearn.lasagne import NeuralNet, BatchIterator, PrintLayerInfo\n",
      "\n",
      "from lasagne.layers import Conv2DLayer as Conv2DLayer\n",
      "from lasagne.layers import MaxPool2DLayer as MaxPool2DLayer"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "Using gpu device 0: GRID K520 (CNMeM is disabled)\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def load_dataset():\n",
      "    # We first define a download function, supporting both Python 2 and 3.                                                                                                                                  \n",
      "    if sys.version_info[0] == 2:\n",
      "        from urllib import urlretrieve\n",
      "    else:\n",
      "        from urllib.request import urlretrieve\n",
      "\n",
      "    def download(filename, source='http://yann.lecun.com/exdb/mnist/'):\n",
      "        print(\"Downloading %s\" % filename)\n",
      "        urlretrieve(source + filename, filename)\n",
      "\n",
      "    # We then define functions for loading MNIST images and labels.                                                                                                                                         \n",
      "    # For convenience, they also download the requested files if needed.                                                                                                                                    \n",
      "    import gzip\n",
      "\n",
      "    def load_mnist_images(filename):\n",
      "        if not os.path.exists(filename):\n",
      "            download(filename)\n",
      "        # Read the inputs in Yann LeCun's binary format.                                                                                                                                                    \n",
      "        with gzip.open(filename, 'rb') as f:\n",
      "            data = np.frombuffer(f.read(), np.uint8, offset=16)\n",
      "        # The inputs are vectors now, we reshape them to monochrome 2D images,                                                                                                                              \n",
      "        # following the shape convention: (examples, channels, rows, columns)                                                                                                                               \n",
      "        data = data.reshape(-1, 1, 28, 28)\n",
      "        # The inputs come as bytes, we convert them to float32 in range [0,1].                                                                                                                              \n",
      "        # (Actually to range [0, 255/256], for compatibility to the version                                                                                                                                 \n",
      "        # provided at http://deeplearning.net/data/mnist/mnist.pkl.gz.)                                                                                                                                     \n",
      "        return data / np.float32(256)\n",
      "\n",
      "    def load_mnist_labels(filename):\n",
      "        if not os.path.exists(filename):\n",
      "            download(filename)\n",
      "        # Read the labels in Yann LeCun's binary format.                                                                                                                                                    \n",
      "        with gzip.open(filename, 'rb') as f:\n",
      "            data = np.frombuffer(f.read(), np.uint8, offset=8)\n",
      "        # The labels are vectors of integers now, that's exactly what we want.                                                                                                                              \n",
      "        return data\n",
      "\n",
      "    # We can now download and read the training and test set images and labels.                                                                                                                             \n",
      "    X_train = load_mnist_images('train-images-idx3-ubyte.gz')\n",
      "    y_train = load_mnist_labels('train-labels-idx1-ubyte.gz')\n",
      "    X_test = load_mnist_images('t10k-images-idx3-ubyte.gz')\n",
      "    y_test = load_mnist_labels('t10k-labels-idx1-ubyte.gz')\n",
      "\n",
      "    # We reserve the last 10000 training examples for validation.                                                                                                                                           \n",
      "    X_train, X_val = X_train[:-10000], X_train[-10000:]\n",
      "    y_train, y_val = y_train[:-10000], y_train[-10000:]\n",
      "\n",
      "    # We just return all the arrays in order, as expected in main().                                                                                                                                        \n",
      "    # (It doesn't matter how we do this as long as we can read them again.)                                                                                                                                 \n",
      "    return X_train, y_train, X_val, y_val, X_test, y_test\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X_train, y_train, X_val, y_val, X_test, y_test = load_dataset()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def build_cnn(input_var=None):\n",
      "    \n",
      "    conv_num_filters = 16\n",
      "    filter_size = 3\n",
      "    pool_size = 2\n",
      "    encode_size = 16\n",
      "    dense_mid_size = 128\n",
      "    pad_in = 'valid'\n",
      "    pad_out = 'full'\n",
      "\n",
      "    # Input layer, as usual:                                                                                                                                                                                \n",
      "    network = InputLayer(shape=(None, 1, 28, 28),\n",
      "                                        input_var=input_var,name=\"InputLayer_0\")                                                                                                                                   \n",
      "    network = Conv2DLayer(\n",
      "            network, num_filters=conv_num_filters, filter_size=(filter_size, filter_size),pad=pad_in,name = \"Conv2DLayer_1\")\n",
      "    \n",
      "    network = Conv2DLayer(\n",
      "            network, num_filters=conv_num_filters, filter_size=(filter_size, filter_size),pad=pad_in,name = \"Conv2DLayer_2\")\n",
      "                                                                                                                                        \n",
      "    network = MaxPool2DLayer(network, pool_size=(pool_size, pool_size),name = \"MaxPool2DLayer_3\")\n",
      "                                                                                                                                     \n",
      "    network = Conv2DLayer(\n",
      "            network, num_filters=2*conv_num_filters, filter_size=(filter_size, filter_size),pad=pad_in,name = \"Conv2DLayer_4\")\n",
      "\n",
      "    network = MaxPool2DLayer(network, pool_size=(pool_size, pool_size),name = \"MaxPool2DLayer_5\")\n",
      "\n",
      "    network = ReshapeLayer(network, shape=(([0], -1)),name = \"ReshapeLayer_6\")\n",
      "    \n",
      "    network = DenseLayer(network,num_units=dense_mid_size,name = \"dense_mid_7\")\n",
      "    \n",
      "    network = DenseLayer(network,num_units=encode_size,name = \"encode\")\n",
      "    \n",
      "    network = DenseLayer(network,num_units=dense_mid_size,name = \"dense_mid_9\")\n",
      "    \n",
      "    network = DenseLayer(network,num_units=800,name = \"dense_mid_10\")\n",
      "    \n",
      "    network = ReshapeLayer(network, shape = (([0], 2*conv_num_filters, 5, 5)),name = \"ReshapeLayer_11\")\n",
      "    \n",
      "    network = Upscale2DLayer(network, scale_factor=(pool_size, pool_size),name = \"Upscale2DLayer_12\")\n",
      "    \n",
      "    network = Conv2DLayer(\n",
      "            network, num_filters=2*conv_num_filters, filter_size=(filter_size, filter_size),pad=pad_out,name = \"Conv2DLayer_13\")\n",
      "    \n",
      "    network = Upscale2DLayer(network, scale_factor=(pool_size, pool_size),name = \"Upscale2DLayer_14\")\n",
      "    \n",
      "    network = Conv2DLayer(\n",
      "            network, num_filters=conv_num_filters, filter_size=(filter_size, filter_size),pad=pad_out,name = \"Conv2DLayer_15\")\n",
      "    \n",
      "    network = Conv2DLayer(\n",
      "            network, num_filters=1, filter_size=(filter_size, filter_size),pad=pad_out, name = \"Conv2DLayer_16\")\n",
      "    \n",
      "    network = ReshapeLayer(network, shape=(([0], -1)),name = \"ReshapeLayer_17\")\n",
      "\n",
      "    return network\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def iterate_minibatches(inputs, targets, batchsize, shuffle=False):\n",
      "    assert len(inputs) == len(targets)\n",
      "    if shuffle:\n",
      "        indices = np.arange(len(inputs))\n",
      "        np.random.shuffle(indices)\n",
      "    for start_idx in range(0, len(inputs) - batchsize + 1, batchsize):\n",
      "        if shuffle:\n",
      "            excerpt = indices[start_idx:start_idx + batchsize]\n",
      "        else:\n",
      "            excerpt = slice(start_idx, start_idx + batchsize)\n",
      "        yield inputs[excerpt], targets[excerpt]\n",
      "\n",
      "def iterator(X, batchsize):\n",
      "    indices = np.arange(len(X))\n",
      "    for i in range(0, len(X) - batchsize + 1, batchsize):\n",
      "        sli = indices[i:i+batchsize]\n",
      "        yield X[sli]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def main(model='cnn', num_epochs=500):\n",
      "    # Load the dataset                                                                                                                                                                                      \n",
      "    print(\"Loading data...\")\n",
      "    X_train, y_train, X_val, y_val, X_test, y_test = load_dataset()\n",
      "\n",
      "    # Prepare Theano variables for inputs and targets                                                                                                                                                       \n",
      "    input_var = T.tensor4('inputs')\n",
      "    #target_var = T.ivector('targets')\n",
      "\n",
      "    # Create neural network model (depending on first command line parameter)                                                                                                                               \n",
      "    print(\"Building model and compiling functions...\")\n",
      "\n",
      "    network = build_cnn(input_var)\n",
      "    laylist = lasagne.layers.get_all_layers(network)\n",
      "    \n",
      "    for l in laylist:\n",
      "        print l.name, lasagne.layers.get_output_shape(l)\n",
      "        \n",
      "    # Create a loss expression for training, i.e., a scalar objective we want                                                                                                                               \n",
      "    # to minimize (for our multi-class problem, it is the cross-entropy loss):                                                                                                                              \n",
      "    prediction = lasagne.layers.get_output(network)\n",
      "    loss = lasagne.objectives.squared_error(prediction, input_var.flatten(2))\n",
      "    loss = loss.mean()\n",
      "    # We could add some weight decay as well here, see lasagne.regularization.                                                                                                                              \n",
      "\n",
      "    # Create update expressions for training, i.e., how to modify the                                                                                                                                       \n",
      "    # parameters at each training step. Here, we'll use Stochastic Gradient                                                                                                                                 \n",
      "    # Descent (SGD) with Nesterov momentum, but Lasagne offers plenty more.                                                                                                                                 \n",
      "    params = lasagne.layers.get_all_params(network, trainable=True)\n",
      "    updates = lasagne.updates.nesterov_momentum(\n",
      "            loss, params, learning_rate=0.01, momentum=0.9)\n",
      "\n",
      "    # Create a loss expression for validation/testing. The crucial difference                                                                                                                               \n",
      "    # here is that we do a deterministic forward pass through the network,                                                                                                                                  \n",
      "    # disabling dropout layers.                                                                                                                                                                             \n",
      "    test_prediction = lasagne.layers.get_output(network, deterministic=True)\n",
      "    test_loss = lasagne.objectives.squared_error(test_prediction,\n",
      "                                                            input_var.flatten(2))\n",
      "    test_loss = test_loss.mean()\n",
      "    # As a bonus, also create an expression for the classification accuracy:                                                                                                                                \n",
      "    test_acc = T.mean(T.eq(T.argmax(test_prediction, axis=1), input_var.flatten(2)),\n",
      "                      dtype=theano.config.floatX)\n",
      "\n",
      "    # Compile a function performing a training step on a mini-batch (by giving                                                                                                                              \n",
      "    # the updates dictionary) and returning the corresponding training loss:                                                                                                                                \n",
      "    #train_fn = theano.function([input_var, target_var], loss, updates=updates)\n",
      "    train_fn = theano.function([input_var], loss, updates=updates)\n",
      "    #out_fn = theano.function([input_var],prediction)\n",
      "    #loss_fn = theano.function([input_var],loss)\n",
      "    # Compile a second function computing the validation loss and accuracy:                                                                                                                                 \n",
      "    #val_fn = theano.function([input_var, target_var], [test_loss, test_acc])\n",
      "    val_fn = theano.function([input_var], [test_loss, test_acc])\n",
      "    # Finally, launch the training loop.                                                                                                                                                                    \n",
      "    print(\"Starting training...\")\n",
      "    # We iterate over epochs:                                                                                                                                                                               \n",
      "    for epoch in range(num_epochs):\n",
      "        # In each epoch, we do a full pass over the training data:                                                                                                                                          \n",
      "        train_err = 0\n",
      "        train_batches = 0\n",
      "        start_time = time.time()\n",
      "            \n",
      "        for batch in iterator(X_train, 500):\n",
      "            #print batch.flatten(2)\n",
      "            #print 'batch.min():', batch.flatten(2).min()\n",
      "            #print 'batch.max():', batch.flatten(2).max()\n",
      "            #print 'batch size:', batch.flatten(2).shape, batch.flatten(2).dtype\n",
      "            #prediction = out_fn(batch)\n",
      "            #print 'pred.min():', prediction.min()\n",
      "            #print 'pred.max():', prediction.max()\n",
      "            #print 'pred size: ', prediction.flatten(2).dtype, prediction.flatten(2).shape\n",
      "            #losses = loss_fn(batch)\n",
      "            #print 'losses.min():', losses.min()\n",
      "            #print 'losses.max():', losses.max()\n",
      "            #print 'losses size: ', losses.dtype, losses.shape\n",
      "            train_err += train_fn(batch)\n",
      "            train_batches += 1\n",
      "            #print train_batches\n",
      "\n",
      "\n",
      "        # And a full pass over the validation data:                                                                                                                                                         \n",
      "        val_err = 0\n",
      "        val_acc = 0\n",
      "        val_batches = 0\n",
      "        #for batch in iterate_minibatches(X_val, X_val.flatten(2), 500, shuffle=False):\n",
      "        #    inputs, targets = batch\n",
      "        #    err, acc = val_fn(inputs, targets)\n",
      "        #    val_err += err\n",
      "        #    val_acc += acc\n",
      "        #    val_batches += 1\n",
      "        \n",
      "        #for batch in iterator(X_val, 500):\n",
      "        #    err, acc = val_fn(batch)\n",
      "        #    val_err += err\n",
      "        #    val_acc += acc\n",
      "        #    val_batches += 1\n",
      "\n",
      "        # Then we print the results for this epoch:                                                                                                                                                         \n",
      "        print(\"Epoch {} of {} took {:.3f}s\".format(\n",
      "            epoch + 1, num_epochs, time.time() - start_time))\n",
      "        print(\"  training loss:\\t\\t{:.6f}\".format(train_err / train_batches))\n",
      "        #print(\"  validation loss:\\t\\t{:.6f}\".format(val_err / val_batches))\n",
      "        #print(\"  validation accuracy:\\t\\t{:.2f} %\".format(\n",
      "        #    val_acc / val_batches * 100))\n",
      "        # After training, we compute and print the test error:                                                                                                                                                  \n",
      "    test_err = 0\n",
      "    test_acc = 0\n",
      "    test_batches = 0\n",
      "    #for batch in iterate_minibatches(X_test, y_test, 500, shuffle=False):\n",
      "    #    inputs, targets = batch\n",
      "    #    err, acc = val_fn(inputs, targets)\n",
      "    #    test_err += err\n",
      "    #    test_acc += acc\n",
      "    #    test_batches += 1\n",
      "    \n",
      "    #for batch in iterator(X_test,500):\n",
      "    #    err, acc = val_fn(batch)\n",
      "    #    test_err += err\n",
      "    #    test_acc += acc\n",
      "    #    test_batches += 1\n",
      "    \n",
      "    print(\"Final results:\")\n",
      "    #print(\"  test loss:\\t\\t\\t{:.6f}\".format(test_err / test_batches))\n",
      "    #print(\"  test accuracy:\\t\\t{:.2f} %\".format(\n",
      "    #    test_acc / test_batches * 100))\n",
      "\n",
      "    # Optionally, you could now dump the network weights to a file like this:                                                                                                                               \n",
      "    # np.savez('model.npz', *lasagne.layers.get_all_param_values(network))                                                                                                                                  \n",
      "    #                                                                                                                                                                                                       \n",
      "    # And load them again later on like this:                                                                                                                                                               \n",
      "    # with np.load('model.npz') as f:                                                                                                                                                                       \n",
      "    #     param_values = [f['arr_%d' % i] for i in range(len(f.files))]                                                                                                                                     \n",
      "    # lasagne.layers.set_all_param_values(network, param_values)  \n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "main()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Loading data...\n",
        "Building model and compiling functions..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "InputLayer_0 (None, 1, 28, 28)\n",
        "Conv2DLayer_1 (None, 16, 26, 26)\n",
        "Conv2DLayer_2 (None, 16, 24, 24)\n",
        "MaxPool2DLayer_3 (None, 16, 12, 12)\n",
        "Conv2DLayer_4 (None, 32, 10, 10)\n",
        "MaxPool2DLayer_5 (None, 32, 5, 5)\n",
        "ReshapeLayer_6 (None, 800)\n",
        "dense_mid_7 (None, 128)\n",
        "encode (None, 16)\n",
        "dense_mid_9 (None, 128)\n",
        "dense_mid_10 (None, 800)\n",
        "ReshapeLayer_11 (None, 32, 5, 5)\n",
        "Upscale2DLayer_12 (None, 32, 10, 10)\n",
        "Conv2DLayer_13 (None, 32, 12, 12)\n",
        "Upscale2DLayer_14 (None, 32, 24, 24)\n",
        "Conv2DLayer_15 (None, 16, 26, 26)\n",
        "Conv2DLayer_16 (None, 1, 28, 28)\n",
        "ReshapeLayer_17 (None, 784)\n",
        "Starting training..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Epoch 1 of 500 took 13.075s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "  training loss:\t\t0.089884\n",
        "Epoch 2 of 500 took 13.039s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "  training loss:\t\t0.074375\n",
        "Epoch 3 of 500 took 13.074s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "  training loss:\t\t0.065102\n",
        "Epoch 4 of 500 took 13.038s"
       ]
      }
     ],
     "prompt_number": "*"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}